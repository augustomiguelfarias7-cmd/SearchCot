""" SearchCot AI — versão completa voltada para consumo por Inteligência Artificial (single-file) License: Apache 2.0

O que este arquivo faz (resumo):

Crawler assíncrono (aiohttp) que extrai texto, links e imagens

Indexador em SQLite (padrão: em memória, mas pode usar arquivo via --db-path)

Ranker TF-IDF + BM25; suporte opcional a sentence-transformers para busca semântica

Summarizer híbrido (extractive, com opção semântica) para gerar visão geral

Endpoints REST voltados para LLM/AGI: GET /ai/search?q=...&k=10&sem=1  -> JSON compacto pronto para parsing por LLMs GET /ai/doc/<id>                 -> Documento estruturado (summary, content truncado, images, links)

CLI com comandos: crawl, build-semantic, serve, search


Dependências: pip install aiohttp beautifulsoup4 flask tqdm numpy scikit-learn

opcional (embeddings/semantic):

pip install sentence-transformers faiss-cpu

Uso rápido:

Crawl e indexar (por padrão banco em memória):

python searchcot_ai_full.py crawl https://example.com --max-pages 100 --db-path :memory:

Rodar servidor AI (API):

python searchcot_ai_full.py serve --host 127.0.0.1 --port 8082 --db-path :memory:

Buscar via CLI:

python searchcot_ai_full.py search "termo" --k 5

Avisos de segurança:

Não exponha o endpoint /ai/* sem autenticação em produção.

O crawler respeita same-domain por default; seja responsável ao rastrear sites. """


import argparse import asyncio import aiohttp import re import sqlite3 import time import os import math from urllib.parse import urljoin, urldefrag, urlparse from bs4 import BeautifulSoup from collections import defaultdict, Counter from tqdm import tqdm import json

Optional imports

try: import numpy as np from sklearn.neighbors import NearestNeighbors has_numpy = True except Exception: has_numpy = False

try: from sentence_transformers import SentenceTransformer has_st = True except Exception: has_st = False

-------------------------------

Config

-------------------------------

DEFAULT_DB = ':memory:' USER_AGENT = 'SearchCotBot/1.0' CRAWL_CONCURRENCY = 8 CRAWL_TIMEOUT = 12 MIN_TEXT_LENGTH = 80 DEFAULT_TOP_K = 10 VISIT_CACHE_TTL = 300

STOPWORDS = set(['o','a','os','as','um','uma','uns','umas','de','do','da','dos','das','e','é','em','no','na','nos','nas','por','com','para','que','como','se','ao','à','às','mais','não']) TOKEN_RE = re.compile(r"\w+", re.UNICODE) SENT_SPLIT_RE = re.compile(r'(?<=[.!?])\s+')

-------------------------------

Tokenization

-------------------------------

def tokenize(text): if not text: return [] tokens = [t.lower() for t in TOKEN_RE.findall(text)] tokens = [t for t in tokens if len(t) > 1 and t not in STOPWORDS and not t.isdigit()] return tokens

-------------------------------

DB

-------------------------------

class IndexDB: def init(self, path=DEFAULT_DB): self.path = path init_needed = False if path != ':memory:': init_needed = not os.path.exists(path) self.conn = sqlite3.connect(path, check_same_thread=False) self.conn.execute('PRAGMA journal_mode=WAL') if init_needed: self._init_db() else: # try to create tables if not exist self._init_db()

def _init_db(self):
    cur = self.conn.cursor()
    cur.executescript('''
    CREATE TABLE IF NOT EXISTS docs (
        id INTEGER PRIMARY KEY,
        url TEXT UNIQUE,
        title TEXT,
        content TEXT,
        images TEXT,
        links TEXT,
        length INTEGER,
        crawled_at REAL
    );
    CREATE TABLE IF NOT EXISTS terms (
        term TEXT PRIMARY KEY,
        doc_freq INTEGER
    );
    CREATE TABLE IF NOT EXISTS postings (
        term TEXT,
        doc_id INTEGER,
        freq INTEGER,
        PRIMARY KEY(term, doc_id)
    );
    CREATE INDEX IF NOT EXISTS idx_postings_term ON postings(term);
    CREATE INDEX IF NOT EXISTS idx_postings_doc ON postings(doc_id);
    ''')
    self.conn.commit()

def add_doc(self, url, title, content, images=None, links=None):
    images = images or []
    links = links or []
    tokens = tokenize(content)
    length = len(tokens)
    cur = self.conn.cursor()
    images_j = json.dumps(images)
    links_j = json.dumps(links)
    try:
        cur.execute('INSERT INTO docs (url,title,content,images,links,length,crawled_at) VALUES (?,?,?,?,?,?,?)',
                    (url, title, content, images_j, links_j, length, time.time()))
        doc_id = cur.lastrowid
    except sqlite3.IntegrityError:
        cur.execute('SELECT id FROM docs WHERE url=?', (url,))
        row = cur.fetchone()
        doc_id = row[0] if row else None
        cur.execute('UPDATE docs SET title=?, content=?, images=?, links=?, length=?, crawled_at=? WHERE id=?',
                    (title, content, images_j, links_j, length, time.time(), doc_id))
        cur.execute('DELETE FROM postings WHERE doc_id=?', (doc_id,))

    freqs = Counter(tokens)
    for t, f in freqs.items():
        cur.execute('INSERT OR IGNORE INTO terms(term, doc_freq) VALUES (?,0)', (t,))
        cur.execute('SELECT 1 FROM postings WHERE term=? AND doc_id=?', (t, doc_id))
        if not cur.fetchone():
            cur.execute('UPDATE terms SET doc_freq = doc_freq + 1 WHERE term=?', (t,))
        cur.execute('INSERT OR REPLACE INTO postings(term, doc_id, freq) VALUES (?,?,?)', (t, doc_id, f))
    self.conn.commit()
    return doc_id

def get_doc(self, doc_id):
    cur = self.conn.cursor()
    cur.execute('SELECT id,url,title,content,images,links,length,crawled_at FROM docs WHERE id=?', (doc_id,))
    row = cur.fetchone()
    if not row: return None
    d = dict(zip(['id','url','title','content','images','links','length','crawled_at'], row))
    d['images'] = json.loads(d['images']) if d['images'] else []
    d['links'] = json.loads(d['links']) if d['links'] else []
    return d

def search_terms_postings(self, terms):
    cur = self.conn.cursor()
    out = {}
    for t in terms:
        cur.execute('SELECT doc_id, freq FROM postings WHERE term=?', (t,))
        out[t] = cur.fetchall()
    return out

def total_docs(self):
    cur = self.conn.cursor()
    cur.execute('SELECT COUNT(*) FROM docs')
    return cur.fetchone()[0]

def get_term_doc_freq(self, term):
    cur = self.conn.cursor()
    cur.execute('SELECT doc_freq FROM terms WHERE term=?', (term,))
    r = cur.fetchone()
    return r[0] if r else 0

def all_docs_meta(self):
    cur = self.conn.cursor()
    cur.execute('SELECT id,url,title,length FROM docs')
    return cur.fetchall()

def close(self):
    self.conn.commit()
    self.conn.close()

-------------------------------

Ranker

-------------------------------

class Ranker: def init(self, index: IndexDB): self.index = index

def tf_idf_scores(self, tokens):
    N = max(1, self.index.total_docs())
    postings = self.index.search_terms_postings(tokens)
    scores = defaultdict(float)
    doc_lengths = {}
    for t in tokens:
        posting = postings.get(t, [])
        df = max(1, self.index.get_term_doc_freq(t))
        idf = math.log((N+1)/df)
        for doc_id, freq in posting:
            scores[doc_id] += freq * idf
            if doc_id not in doc_lengths:
                doc = self.index.get_doc(doc_id)
                doc_lengths[doc_id] = doc['length'] if doc else 0
    for d in list(scores.keys()):
        L = max(1, doc_lengths.get(d,1))
        scores[d] /= math.sqrt(L)
    return scores

def bm25_scores(self, tokens, k1=1.5, b=0.75):
    N = max(1, self.index.total_docs())
    postings = self.index.search_terms_postings(tokens)
    scores = defaultdict(float)
    docs = self.index.all_docs_meta()
    avgdl = sum([r[3] for r in docs]) / max(1, len(docs))
    for t in tokens:
        posting = postings.get(t, [])
        df = max(1, self.index.get_term_doc_freq(t))
        idf = math.log(1 + (N - df + 0.5)/(df + 0.5))
        for doc_id, freq in posting:
            doc = self.index.get_doc(doc_id)
            dl = doc['length'] if doc else 0
            denom = freq + k1*(1 - b + b*(dl/avgdl))
            score = idf * (freq*(k1+1))/max(1e-9, denom)
            scores[doc_id] += score
    return scores

-------------------------------

Crawler

-------------------------------

class Crawler: def init(self, db: IndexDB, max_pages=200, max_depth=2, same_domain=True, concurrency=CRAWL_CONCURRENCY): self.db = db self.seen = set() self.to_visit = asyncio.Queue() self.session = None self.max_pages = max_pages self.max_depth = max_depth self.same_domain = same_domain self.concurrency = concurrency self.count = 0

async def _fetch(self, url):
    try:
        async with self.session.get(url, timeout=CRAWL_TIMEOUT) as resp:
            if resp.status != 200:
                return None
            text = await resp.text(errors='ignore')
            return text
    except Exception:
        return None

def _extract(self, base_url, html):
    soup = BeautifulSoup(html, 'html.parser')
    title = soup.title.string.strip() if soup.title and soup.title.string else ''
    for s in soup(['script','style','noscript']):
        s.decompose()
    text = re.sub(r'\s+', ' ', soup.get_text(separator=' ')).strip()
    links = []
    for a in soup.find_all('a', href=True):
        href = a['href']
        href = urljoin(base_url, href)
        href = urldefrag(href)[0]
        links.append(href)
    images = []
    for img in soup.find_all('img', src=True):
        src = img['src']
        src = urljoin(base_url, src)
        images.append(src)
    return title, text, links, images

async def worker(self, origin_host):
    while self.count < self.max_pages:
        try:
            url, depth = await asyncio.wait_for(self.to_visit.get(), timeout=1.0)
        except asyncio.TimeoutError:
            return
        if url in self.seen:
            self.to_visit.task_done()
            continue
        if self.same_domain and urlparse(url).hostname != origin_host:
            self.to_visit.task_done()
            continue
        self.seen.add(url)
        html = await self._fetch(url)
        if not html:
            self.to_visit.task_done()
            continue
        title, text, links, images = self._extract(url, html)
        if len(text) >= MIN_TEXT_LENGTH:
            self.db.add_doc(url, title, text, images=images, links=links)
            self.count += 1
        if depth < self.max_depth:
            for l in links:
                if l not in self.seen:
                    await self.to_visit.put((l, depth+1))
        self.to_visit.task_done()

async def crawl(self, seeds):
    connector = aiohttp.TCPConnector(limit=self.concurrency)
    headers = {'User-Agent': USER_AGENT}
    async with aiohttp.ClientSession(connector=connector, headers=headers) as session:
        self.session = session
        origin_host = urlparse(seeds[0]).hostname if seeds else None
        for s in seeds:
            await self.to_visit.put((s, 0))
        workers = [asyncio.create_task(self.worker(origin_host)) for _ in range(self.concurrency)]
        await self.to_visit.join()
        for w in workers:
            w.cancel()

-------------------------------

Semantic (optional)

-------------------------------

class SemanticIndex: def init(self, db: IndexDB, model_name='all-MiniLM-L6-v2'): if not has_st or not has_numpy: raise RuntimeError('sentence-transformers or numpy not available') self.db = db self.model = SentenceTransformer(model_name) self.embeddings = None self.doc_ids = None self.nn = None

def build(self):
    rows = self.db.all_docs_meta()
    texts = []
    ids = []
    for doc_id, url, title, length in rows:
        d = self.db.get_doc(doc_id)
        if not d: continue
        texts.append((d.get('title','') or '') + '\n' + (d.get('content','') or ''))
        ids.append(doc_id)
    if not texts:
        self.embeddings = np.zeros((0,1))
        self.doc_ids = []
        return
    embs = self.model.encode(texts, show_progress_bar=True)
    self.embeddings = np.array(embs, dtype=np.float32)
    self.doc_ids = ids
    self.nn = NearestNeighbors(n_neighbors=10, metric='cosine')
    self.nn.fit(self.embeddings)

def query(self, q, k=10):
    if self.nn is None:
        self.build()
    q_emb = np.array(self.model.encode([q]), dtype=np.float32)
    dists, idxs = self.nn.kneighbors(q_emb, n_neighbors=k)
    results = []
    for dist, idx in zip(dists[0], idxs[0]):
        results.append((self.doc_ids[idx], 1 - float(dist)))
    return results

-------------------------------

Summarizer

-------------------------------

def summarize_text_hybrid(text, top_k=3): sentences = [s.strip() for s in SENT_SPLIT_RE.split(text) if s.strip()] if not sentences: return '' token_scores = Counter(tokenize(text)) sent_scores = [] for s in sentences: toks = tokenize(s) score = sum(token_scores.get(t,0) for t in toks) sent_scores.append((score, s)) sent_scores.sort(reverse=True) top = [s for _, s in sent_scores[:top_k]] if has_st and has_numpy: try: model = SentenceTransformer('all-MiniLM-L6-v2') emb = model.encode(sentences) centroid = emb.mean(axis=0, keepdims=True) from sklearn.metrics.pairwise import cosine_similarity sims = cosine_similarity(emb, centroid).flatten() idxs = sims.argsort()[::-1][:top_k] sem_top = [sentences[i] for i in idxs] combined = [] for s in sentences: if s in top or s in sem_top: if s not in combined: combined.append(s) return ' '.join(combined[:top_k]) except Exception: return ' '.join(top) return ' '.join(top)

-------------------------------

Search Engine wrapper

-------------------------------

class SearchEngine: def init(self, db_path=DEFAULT_DB): self.db = IndexDB(db_path) self.ranker = Ranker(self.db) self.semantic = None

def ensure_semantic(self):
    if not has_st or not has_numpy:
        return False
    if not self.semantic:
        self.semantic = SemanticIndex(self.db)
        self.semantic.build()
    return True

def search(self, q, k=DEFAULT_TOP_K, use_semantic=False):
    tokens = tokenize(q)
    tf = self.ranker.tf_idf_scores(tokens)
    bm = self.ranker.bm25_scores(tokens)
    combined = defaultdict(float)
    for d,s in tf.items(): combined[d] += 0.4*s
    for d,s in bm.items(): combined[d] += 0.6*s
    if use_semantic and self.ensure_semantic():
        sem = self.semantic.query(q, k=50)
        for doc,score in sem:
            combined[doc] += 2.0*score
    ranked = sorted(combined.items(), key=lambda x: x[1], reverse=True)[:k]
    out = []
    for doc_id,score in ranked:
        d = self.db.get_doc(doc_id)
        snippet = make_snippet(d.get('content','') if d else '', tokens)
        out.append({'id':doc_id,'url':d.get('url','') if d else '','title':d.get('title','') if d else '', 'score':float(score),'snippet':snippet,'images':d.get('images',[]) if d else [], 'links': d.get('links',[]) if d else []})
    return out

-------------------------------

API (Flask)

-------------------------------

from flask import Flask, request, jsonify

def create_ai_app(engine: SearchEngine): app = Flask('searchcot_ai')

@app.route('/ai/search')
def ai_search():
    q = request.args.get('q','').strip()
    if not q:
        return jsonify({'error':'missing q parameter'}), 400
    k = int(request.args.get('k', DEFAULT_TOP_K))
    sem = request.args.get('sem') is not None
    res = engine.search(q, k=k, use_semantic=sem)
    # compact payload for LLM consumption
    compact = []
    for r in res:
        # generate a short summary for LLM
        summary = ''
        if r.get('snippet'):
            summary = r['snippet']
        elif r.get('title'):
            summary = r['title']
        compact.append({'id': r['id'], 'url': r['url'], 'title': r['title'], 'score': r['score'], 'snippet': r['snippet'], 'images': r.get('images',[]), 'links': r.get('links',[]), 'summary': summary})
    return jsonify({'query': q, 'results': compact})

@app.route('/ai/doc/<int:doc_id>')
def ai_doc(doc_id):
    d = engine.db.get_doc(doc_id)
    if not d:
        return jsonify({'error':'doc not found'}), 404
    content = re.sub(r'\s+', ' ', d.get('content','')).strip()
    max_len = 4000
    if len(content) > max_len:
        content = content[:max_len] + '...'
    summary = summarize_text_hybrid(d.get('content',''), top_k=4)
    return jsonify({'id': d['id'], 'url': d['url'], 'title': d['title'], 'summary': summary, 'content': content, 'images': d.get('images',[]), 'links': d.get('links',[])})

return app

-------------------------------

CLI

-------------------------------

def make_argparser(): p = argparse.ArgumentParser(description='SearchCot AI — AI-first search engine') sub = p.add_subparsers(dest='cmd')

pc = sub.add_parser('crawl')
pc.add_argument('seeds', nargs='+')
pc.add_argument('--max-pages', type=int, default=200)
pc.add_argument('--max-depth', type=int, default=2)
pc.add_argument('--same-domain', action='store_true')
pc.add_argument('--concurrency', type=int, default=CRAWL_CONCURRENCY)
pc.add_argument('--db-path', default=DEFAULT_DB)

ps = sub.add_parser('search')
ps.add_argument('query', nargs='+')
ps.add_argument('--k', type=int, default=10)
ps.add_argument('--db-path', default=DEFAULT_DB)

pb = sub.add_parser('build-semantic')
pb.add_argument('--db-path', default=DEFAULT_DB)

psrv = sub.add_parser('serve')
psrv.add_argument('--host', default='127.0.0.1')
psrv.add_argument('--port', default=8082, type=int)
psrv.add_argument('--db-path', default=DEFAULT_DB)

return p

def main(): parser = make_argparser() args = parser.parse_args() if args.cmd == 'crawl': db = IndexDB(args.db_path) c = Crawler(db, max_pages=args.max_pages, max_depth=args.max_depth, same_domain=args.same_domain, concurrency=args.concurrency) print('Starting crawl...') asyncio.run(c.crawl(args.seeds)) print('Crawl finished. Docs:', db.total_docs()) db.close() elif args.cmd == 'search': q = ' '.join(args.query) engine = SearchEngine(db_path=args.db_path) res = engine.search(q, k=args.k) print(json.dumps({'query': q, 'results': res}, ensure_ascii=False, indent=2)) elif args.cmd == 'build-semantic': engine = SearchEngine(db_path=args.db_path) print('Building semantic index (requires sentence-transformers + numpy) ...') try: engine.ensure_semantic() print('Semantic built') except Exception as e: print('Semantic build failed:', e) elif args.cmd == 'serve': engine = SearchEngine(db_path=args.db_path) app = create_ai_app(engine) print(f'SearchCot AI serving at http://{args.host}:{args.port}') app.run(host=args.host, port=args.port) else: parser.print_help()

if name == 'main': main()

